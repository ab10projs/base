1)
2) Grouping and displaying
2.1) Data Array
2.2) Frequency distribution is a better way of data display w.r.t arrays
2.2.1) Classes of some interval are created and the count of elements (frequency) is callacualed
2.2.2) Relative frequency distribution has the % or values of class, and add to 100% for all classes
2.2.2.1) Steps for frequency distribution
a) Decide number of classes
b) calculate the width of class
c) sort the values and count number of entries per class
d) Histograms and Frquency polygones.. Ogives show relative frequency


3) Measures of entral Tendency and Dispersion in Frequency
3.1) Central Tendency: Middle point of a distribution
3.2) Dispersion is spread of data (level to which the values are scattered
3.3) skewness: (Symmetrical or skewed). Skewed to left means long tail is on left side.
3.4) Kurttois: Peakedness. 
3.5) Charactersticts of samples are called statistics
3.6) Charactersticts of population are called parameters

#---------Ungrouped Data starts -------------#
3.7) μ = Σx/N (population) , where (N is number of elements in population)  and (Σx is sum of values of all observations)
3.8) x̄ = Σx/n (sample)
3.9) Disadvantages of Averages
1) impacted by extream values
2) tedious for large values
3) not possible in open interval classes
#---------Ungrouped Data   ends -------------#
 
#---------Grouped Data starts -------------#
3.10) When the number of observations are large, then for ease of calculation some accuracy is given up by gouping values in classes
Step1) We decideon the number of classes in which all values are to be divided.
Step2) We find mid point of each class
Step3) We multiply mid point with frequency of each class, where frequency is the number of values in each class.
x̄ = Σ(frequencyOfEachClass*midPointOfClass)/numberOfObservations             .. here numberOfObservations = ΣfrequencyOfEachClass

Weighted mean given importance to each value based on its weight
x̄weighted = Σ(w*x)/w    .. where w is weight assigned to each observation


as we know .. If  aPOWERn = b  then  a = n√b
Geometric Mean
Sometimes values change over time. We need to know "average rate of change" .. like growth rate. 
GM = n√(a1*a2*a3*.... an)
used with ratios, %change, rate of change




Harmonic mean
square(GM) = AM * HM
AM = (a+b)/2
GM = √(a*b)
HM = 2*ab/(a+b)
HM= n/[(1/r1)+(1/r2)+(1/r3)]

Medean
(n+1)/2

Mode
Mo= LowLimOfModalClass + (d1/(d1+d2))w 
here w is width
d1= frequecy of modal class - frequency of modela class directly below it
d2= frequecy of modal class - frequency of modela class directly above it


Range
diffrece between lowest and highest value
fractile divides all values in 3 parts
quartile divides all values in 4 parts
decile   divides all values in 10 parts

#---------Grouped Data   ends -------------#


DISPERSION
It gives additional information of how reliable is the central tendency value. If the dispersion is wide, then central value is not very strong representative of data

UNGROUPED DATA
σ2 = Σ ( Xi – μ )2 / N
σ = Population Standard Deviation
z Population standard score = (x-μ)/σ

Variance of GROUPED DATA
σ2 = Σf(x-μ)squared/N


SAMPLE Variance
sPower2 = Σ (x-x̄)Power2  / n-1
s = Sample Standard deviation

Standard score tells how many standard deviations away is any value from its mean

SDs of two samples cannot be compared, so we have to convert SD in terms if percentage.
Cofficiant of variation expresses SD as percent of mean. It is a ratio so it does not have a unit.
Population covariance = σ/μ

#------------------------------------------------------------------------------------------------------------------------------

#----------------------  Probability starts ------------------------------------------------------
There are two broad types of PROBLEMS
A) Deterministc (frequency distribution or discriptive statistics is used)
B) Probabilistic (Random)  (Probability and probability distributions are used)

Sample Space: It is a set of all outcoms
Event: A desirable outcome
Experiment: An activity that produces an Event
Mutually Exclusive: When only one of the events can occur, and other can not.
Collectively Exhaustive: All possible outcomes

There are THREE TYPES OF CLASSIFYING PROBABILITIES
1) CLASSICAL APPROACH (also know as priori)
2) RELARIVE FREQUENCY APPROACH
3) SUBJECTIVE APPROACH

Classical Probability:
Probability of an Event = (number of outcomes where event occured)/total number os events  ...  Here each outcome is equally likely
It is also called prori as we can tell the answer without actually doing experiment. This approach assumes very orderly situations and may not be fit for many real life
Here WITH REPLACEMENT OR WITHOUT REPLACEMENT is important.


RELATIVE FREQUENCY:
If an even is capable of repeating itself with a frequency of 'f' in sufficiently large(N) trials, then relative frequency = f/N
The observed relative frequecy of an event in very large number of trials. The proportion of times an event occurs in long run under stable conditions.
The relative frequency becomes stable when number of experiments become very large.
Here it is required to do large number of experiments.
The cost of experiments may increase ,so we need to be careful.
In this category, the assumption of all events being equally likely and mutually exclusive is not present.
Disadvantage is that this approach assumes the event should repeat itself after some trials and the environmental conditions should be same(which is not realistic).
It is also not possble to know that after how many occurances, the event stabalizes.
In real life, events many not have occured many times and data may not be available to use it.

SUBJECTIVE APPROACH:
These are probabilities assigned to events by person based on educated guess or any evidence available.


P(A) means probability of event A happening.  It is also called marginal or unconditional probability.
ADDITIONAL RULE OF PROBABILISTIC EVENTS If two events are NOT mutually exclusive : P(A or B) = P(A) + P(B) - P(AB)  .. This -P(AB) is to remove double count
ADDITIONAL RULE OF PROBABILISTIC EVENTS If two events ARE mutually exclusive : P(A or B) = P(A) + P(B)

#----STATISTICAL INDEPENDENCE  (conditional,joint, marginal) starts------------------------------------------------
MARGINAL PROBABILITIES UNDER STATISTICAL INDEPENDENCE:
P(A) , as it has no impact of previous or any other event

JOINT PROBABILITIES UNDER STATISTICAL INDEPENDENCE:
The probability of two or more events happening together or in succession is product of their marginal probability.
Joint probability of two independent events:  P(AB) = P(A) * P(B)  ... Here P(AB) is probability of event A and B happening together or in succession.
e.g probability of getting 3heads is 0.5*0.5*0.5 = 0.125

CONDITIONAL PROBABILITIES UNDER STATSTICAL INDEPENDENCE:
P(B|A)  .. Probability of event B given that event A has happened
Conditional: P(B|A) = P(B) if B and A are two independent events
#----STATISTICAL INDEPENDENCE  (conditional,joint, marginal) ends--------------------------------------------------


#----STATISTICAL DEPENDENCE  (conditional,joint, marginal) starts-----------------------------------------------------------
CONDITIONAL PROBABILITIES UNDER STATSTICAL DEPENDENCE:  
P(B|A) = P(BA)/P(A) .... This means probability of B happening given A has occured is joint probability of AB divided by probability of A

JOINT PROBABILITIES UNDER STATSTICAL DEPENDENCE:
taking reference from formula of CONDITIONAL PROBABILITIES UNDER STATSTICAL DEPENDENCE, which is  P(B|A) = P(BA)/P(A), we get --> P(BA) = P(B|A)*P(A)

MARGINAL PROBABILITIES UNDER STATSTICAL DEPENDENCE:
P(A) ... Sum all probabilities of joint events where A happened.
#----STATISTICAL DEPENDENCE   (conditional,joint, marginal)  ends-----------------------------------------------------------

BAYES Theorem:
offers powerful statistical method of evaluating new information and revising prior estimates.
P(B|A) = P(BA)/P(A)   .. same as CONDITIONAL PROBABILITIES UNDER STATSTICAL DEPENDENCE..

REVISING PRIOR ESTIMATES WITH BAYES THEOREM
Additional information may result in need to revise probabilities. These new probabilites are called POSTERIOR probabilities
It makes possible to avoid collection of massive data over period of time to make good decision based on probability.
  
example: two die with .4 and .7 chances of showing 1 dot
#----------------------  Probability ends ------------------------------------------------------

#----------------------- Probability Distribution Starts --------------------------------------------
Probability distribution can be thought to be theoritical frequency distribution. A theoritical frequency distribution is how outputs are EXPECTED to appear.
(example , if we take two coin and plot the frequency distribution of H, then (TT,TH,HT,HH) for 0(1occurance),1(2occurance),2(1occurance) is probability distribution.
That means, a probability distribution is a listing of all "possible" outcomes that "could" result if the experiment was "actually" done.

Probability distribution is linked to frequency distribution. 
A frequency distribution is listing of ALL "observed" frequencies of all outomes of an experiment that "actually" occured, whereas probability distribution is a listing
of probabilities of all possible outcomes that COULD theoritically result.


TYPES OF PROBABILITY DISTRIBUTIONS:
Probability ditributions are classified as DISCRETE or CONTINOUS
In discrete distribution, the number of possible output are fixed values and hence all possible values can be listed

In continous distribution, infinite number of outputs are possible, so these can not be listed.
Continous distribitions are convenient way of representation of discrete distributions that have millions of possible values.

RANDOM VARIABLE:
A varible that takes different values as a result of outcome of a random experiment in NO PREDICTABLE SEQUENCE.
If this variable is allowed to take only limited values, then it is called discrete random variable
If this variable is allowed to take any value wihin a range, then it is called continous random variable.

 
PROBABILITY DISTRIBUTION OF DISCRETE RANDOM VARIABLE:
The frequency of various outomes (like number of patients screened per day) for say last 100 days is considered as typical, 
we can use these historical records to assign a probability to each frequency and get a probability distribution. Here we
assume that the trend of these 100 days will repeat. Here 100 observaions are taken. The frequency of each value of "number of patients" is used to get %age of that
number and sumtotal of all these frequencies is = 100

Expected value of discrete random variable is obtained by multiplying each outcome by its %probability and summing it.
Expected value of a discrete random variable is weighted average of each possible outcome, multiplied by probability of
that outcome.

USE OF EXPECTED VALUE IN DECISION MAKING
cherry box and each day's demand example


BINOMIAL DISTRIBUTION (BERNOULLI PROCESS):
Used for discrete (not continous) random variables. 

CONDITIONS WHEN BERNOULLI PROCESS CAN BE USED:
1) Each trial has only 2 outcomes
2) Probability of the outcome of any trial remains fixed over time
3) Trials are statistically independent.
p = probability of success
q = probability of failure
r = number of successes desired
n = number of trial 

Probability of r success in n trials  = [(n)!/r!*(n-r)!] * (p power(r)) * (q power (n-r))
(example: 0,1,2,3,4,5 employees coming late)

Graphical observations of binomial distribution:
a) when p is small (.1), the distribution is scewed to right (long tail at right)
b) when p increases to .3, the scew is less noticable
c) when p=.5, the distribution is symetrical
d) when p is large ,  the distribution is scewed to left

There is a binomial probability TABLE where for p,n,r combination, the probability is found...

CENTRAL TENDENCY AND DISPERSION FOR BINOMIAL DISTRIBUTION:
Mean of binomial distibution -->  μ = np
σ = √(npq)


POISSION DISTRIBUTION:


(example: number of patients arriving in an interval of time)
properties of POISSON distribution
a) mean number of events occuring per interval of time can be obtained from historic data
b) If the time is divided into very small interval(say 1 sec) then below are true:
b.1) The probability of event happening per sec(small interval) is constant and only 1 event can happen in this interval (e.g only 1 patient can come per sec)
b.2) The probability of 2 patiests comming per sec is so small that it is assigned 0
b.3) The number of patients coming per sec are independent of when that 1 sec happens in full duration of 1 hr
b.4) The number of patients arriving per sec are independent of number of patients arriving at any other second

p(x) =  [(𝝺 power x ) * (e power -𝝺) ]/ x!

p(x) = probability of exactly x occcurances
𝝺 = mean number of occurances per unit time 
(
ex: if 5 accidents happen per month at a crossing, then 𝝺 =5 and x=0
probability of 0 accident = p(0) = [ (5 power 0)*(e power -5)]/0!
probability of 2 accident = p(2) = [ (5 power 2)*(e power -5)]/2! 
)


NORMAL DISTRIBUTION:
Continous random variable distribution.
Properties of Normal Distribution:
	Has single peak
	Mean, Median and Mode overlap
	Two tails never touch x 

There are FAMILIES of Normal curves ...  not just one.
μ and  σ are needed to describe a normal distribution


SOME QUICK POINTS
a) for binomial distribution, if (n =>20 AND np <=0.5, then POISSON can be approximated
b) for binomial distribution, if (np =>5 AND nq <= 5, then Normal distribution cane be used with μ=np and σ = √(npq)



#----------------------- Probability Distribution Ends   --------------------------------------------


#---------------------- Sampling and Sampling distribution starts ------------------------------------------
when all population elements are studied, it is called census. or complete enumeration
Sampling is used when al elements can not be studied.
Sample is a portion of population
Sample----STATISTICS is characterstics of sample and PARAMETER is characterstics of population.
Two types of Sampeling (NonRandom OR Judgemental)    AND   (Random or probability)

Biased Samples: When sample is selected that is not proper representaton of population
There are 4 types of Random Sampling:
a) Simple Random sampling: each sample has equal probability of selection and each element has equal prob of getting selected in a sample
b) systematic random sampling: Selected at uniform interval.. may have low cost BUT may inject bias.
c) Stratified sampling:Population is divided into homogenous groups called strata. 	
d) clustered sampling


If we take various samples from a populaton and find the mean and sd of each sample. then the sd and mean of each would be different. A probability distribution of
"all possible means" of the "samples" is a "distribution of sample mean".  It is called sampling distribution of mean.

We can also find sampling distribution of proportions.
STANDARD ERROR (σx̄ = σ/√(n)):
Standard deviation of distribution of standard means is called STANDARD ERROR OF MEANS.  
Siilarly, standard deviation of distribution of proportions  is called standard error of proportion.
Standard error gives an idea about that accuracy we are likely to get if we use sample statistices to estimate population parameters. For populations
which have small spread, the sample means are likely to be having small spread and hence accruately estimate population.
e.g (standard error of mean, standard error of proportion, standard error of median

SAMPLING FROM NORMAL POPULATIONS:
Say the population mean = 100 and SD =25
If we take samples of 5 items each, then the chances of sample mean to be above population mean is equal to chance of it beimg below population mean.
We would get less spread in sample means then the spread of individual items.
In other words standard error of "Sampling distribution of means" is less than standard deviation of individual items in the population.
1) we calculate standard error as ..σx̄ = σ/√(n)
2) we calculate z = (x-μ)/σx̄ 
3) we then calculate the area under the curve to get the probability



SAMPLING FROM NON-NORMAL POPULATIONS:
if we have small sample size (say 5 entries), the also, we can take combinations out of these 5 and average them to get 20 entries. 
If we plot these, we will see normal distribution.

CENTRAL LIMIT THEOREM
a) mean of sampling distribution of means will equal to population mean
b) as sample size increases, distribution becomes normal, regardless of shape of population.

statisticians use normal distribution as pproximation to sampling distribution whenever sample size is 30 or more
Central limit theorm lets us use sample statistics to estimate population paramenters without knowing anything about population

SAMPLING DISTRIBUTION OF PROPORTIONS
^p = x/n   (^p is sample proportion, x is total occurances and n is sample size)

As per binomial distribution, Mean = np   and  σx = √(npq)
so Mean of sample proportion ^p = np/n = p
and sd of proporion σ^p = √(npq)/n  OR  √(pq/n)
z or proportion =  (^p - p)/ ( √(pq/n) )      


FINITE POPULATION MULTIPLIER
FINITE population:
σx̄ = σ/√(n) * [(N-n)/(N-1)]
#---------------------- Sampling and Sampling distribution   ends ------------------------------------------

#---------------------- Estimation starts ---------------------------------#
TYPES OF ESTIMATES:

POINT: Single number to estimate population parameter. It is either correct or incorrect.
INTERVAL:  A range of values to estimate population parameter.

ESTIMATOR AND ESTIMATES:
ESTIMATOR: sample statistics that is used to estimate population parameter. e.g sample mean.
ESTIMATE: specific observed value. (Unbiased, efficient, consitent, Sufficient)

VARIANCE OF SAMPLE MEAN: Σ(x-x̄)power2/(n-1)
STANDARD DEVIATION of SAMPLE MEAN: √[ Σ(x-x̄)power2/(n-1) ]

	 
INTERVAL ESTIMATE:
A range of values in which the population parameter is likely to lie.
1) calculate the standard error
2) calculate the sample mean
3) sample meanPlus minus "standard error" gives the interval estimate (with 68.3% confidence)

The probability of 95.5 means that is we take 1000 samples, and construct intervals of plusminus 2 SD around mean, then 995 of these intervals will 
include the population mean.

A statement "we are 95% confident that the life of battries is between 38 and 42 months" DOES NOT mean that "chance is 95% that mean life of ALL battries fall within
interval established from this one sample" .... Rather , it means: If we take MANY samples of same size and construct confidence interval of each sample, then in
95% of the samples the interval will contain population mean.


CALCULATING INTERVAL MEAN FROM LARGE SAMPLES WHERE POPULATION SD(σ) IS KNOWN
when n>30, we can assume normal distribution and  then:
1) calculate standard error from population SD (which is known).
2) use sample mean plusminus StandardError to get confidence interval


WHEN POPULATION STANDARD DEVIATION IS UNKNOWN (σ is unknown)
here n(sample size>30), x̄ (sample mean) and, s(sample sd) are known
1) since n>30, normal distribution is assumed.
2) derive population SD from sample ^σ = s = √[ Σ(x-x̄)power2/(n-1) ]
3) we calculate Estimated standard error of mean of finite population  = ^σx̄ =  Σ(x-x̄)power2/(n-1)  * [(N-n)/(N-1)]
4) use x̄ plusminus 1.64*(^σx̄)  to get 95% confidence interval



INTERVAL ESTIMATE OF PROPORTIONS
samples are used to know proportion of occurances in population.(example. proportion of unemployed people in population.
Binomial distribution is correct distribution for proportion
If np and nq are >= 5 then normal distribution can be a good approximation of binomial distribution.
for a sample: 
μ(p̄) = p
σ(p̄) = √((pq)/n)
^σ(p̄) = √((p̄q̂)/n)

confidence interval = p̄ plusminus (^σ(p̄))


t DISTRIBUTION (WHEN N <30 AND population Standard Deviation is unknown  AND population is assumed to be normal)
t distribution is flatter than normal distribution. It is lower at mean and higher at tails
as n increases, t distribution tends to become normal distribution.
#---------------------- Estimation   ends ---------------------------------#


# -------------------- Hypotheses testing -- 1 sample test -----------------------------#
Hypotheses begins with an assumption about population parameter. Then sample statistics are collected and it is seen how likely is that the 
population parameter assumption is correct based on the sample statistics.

The decision to chose acceptable range (i.e. 2% or 5%) depends on if we can accept the risk or rejecting a correct population hypotheses.There is no risk-free tradeoff

TESTING HYPOTHESES
1) State the assumed hypotheses  H0 called "Null Hypotheses".. example H0:μ= 500   (population mean is 500)
2) "Alternate hypotheses" H1 called is decided. It can be H1:μ !=500 ,  OR   H1:μ<500   OR   H1:μ>500
3) The chance of occurance of difference between sample mean and hypothesized mean is found (say 4.5%). This 4.5% is called "Significance level"
If hypotheses is true, then significance level indicates % of samples having means outside the acceptance limit. At 5% of significance level, the 
area under BOTH tails will be 2.5% each if two tail test is done.
There is no universal standard significance level.

NOTE: Even if the sample mean is lies in acceptable region, then it DOES NOT PROVE H0 IS TRUE. IT MEANS THERE IS NO STATISTICAL EVIDENCE TO REJECT IT

HIGHER THE SIGNIFICANCE LEVEL, HIGHER IS THE CHANCE OF REJECTING IT WHEN H0 IS INFACT TRUE.
Type 1 ERROR: Reject H0 when it is true and its probability is called alpha(or significance level).. Once we decide significance level(alpha) we can not do anything about type1
Type 2 Error: Accept H0 when it is false and its probability is called beta
TO get low beta, we have to make alpha high.

4) Now decide which distribution should be used (Normal, or t).

POWER OF HYPOTHESES TEST
(1- beta) on Y   V/s  Various population means on X


# -------------------- Hypothesis testing -- 1 sample test -----------------------------#

# -------------------- Hypothesis testing -- 2 sample test -----------------------------#
To know if parameters from two populations are alike ? Here the RELATION is of interest and not actual value.
As we are interested in studing two populations, sampling distribution is "sampling distribution of difference of sample means (x̄1 -x̄2)"

POPULATION1: (μ1 mean , σ1 SD , σx̄1 Standard Error , x̄1 sample mean)...  (μx̄1 is "distribution of all possible values of x̄1").... μx̄1 =  μ1   
POPULATION2: (μ2 mean , σ2 SD , σx̄2 Standard Error , x̄2 sample mean)...  (μx̄2 is "distribution of all possible values of x̄2").... μx̄2 =  μ2 

We plot Sampling distribution of "difference between sample means"(x̄1-x̄2)
Mean of "sampling distribution of difference between sample means is" :  μ(x̄1 -x̄2)
Also, μ(x̄1 -x̄2) = μ(x̄1) - μ(x̄2) = μ1 - μ2

Variation of population1 : (σ²1/n1)
Variation of population2 : (σ²2/n2)
So σ(x̄1 -x̄2) =  √( (σ²1/n1) + (σ²2/n2) ) 

standard error of "difference between sample means" is represented by σ(x̄1 -x̄2)

σ²
s²
 

# -------------------- Hypothesis testing -- 2 sample test -----------------------------#
# -------------------- Chi Square and Analysis of Varaiance ANOVA ---------------------------#
If we want to know if more that 2 population proportions can be considered equal, we use chi-square test
chi-square can do much more than comparing if more than two proportions can be considered equal
If we classify the population into several categories based on two attributes(age, performance), we can use chi-square to check if these two attribures are
independent of each other.
If we have more that two population means, then ANOVA enables us to test if more than 2 population means can be considered equal.

CHI-SQUARE, A TEST OF INDEPENDENCE
If we are interested in knowing if differences in several sample proportions are significant or just matter of chance.

TEST OF GOODNESS FIT:
chi-square test helps in finding if normal, binomial, poisson distribution is is appropriate. Whether there is a signifiant disfference between an observed
refquency distribution and theoritical frequency distribution.


DoF Degree of freedom:
find number of classes (k), then k-1 is DoF. After this, if population mean is also to be estimated from sample mean, then subtract 1 more. After this, if 
the population SD is also to be estimated from sample sd, then subtract 1 more from DoF.


AVOVA: Significance of variance among more than 2 sample means
We can find if our samples are drawn from same population or not.

1) We calculate Between column variance
2) Calculate within column variance
3) F = (between-column variance)/(within column varaince)
when populations are not same, between-column variance > withon-column variance , which results in rejection of null-hypotheses
4) Calculate degree of freedom = number of columns -1
5) Use F table to find the value with significance level beyond which area under curve is in rejection region.
  


# -------------------- Chi Square and Analysis of Varaiance ANOVA ---------------------------#

# ----------------- Simple Regression and correlation----------------------#
To learn specific relation between wo or more variables
To use regression analysis to estimate the relation between two variables
Use least square estimating equation to predict future values of a varaible
How correlation analysis describes degree to which two variables are related to each other
Use coefficient of determination to find the streangth or relation between two variables.
We want to learn: How to determine relation between two variables.
Chi-square tells if there exists a relation between two variables, but it does not tell what relation exists. Regression and correlation tell us both the 
nature and strength of relation between two variables.

In regression analysis we will establish an "estimating equation"




# ----------------- Simple Regression and correlation ----------------------#

















  