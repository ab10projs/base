1)
2) Grouping and displaying
2.1) Data Array
2.2) Frequency distribution is a better way of data display w.r.t arrays
2.2.1) Classes of some interval are created and the count of elements (frequency) is callacualed
2.2.2) Relative frequency distribution has the % or values of class, and add to 100% for all classes
2.2.2.1) Steps for frequency distribution
a) Decide number of classes
b) calculate the width of class
c) sort the values and count number of entries per class
d) Histograms and Frquency polygones.. Ogives show relative frequency


3) Measures of entral Tendency and Dispersion in Frequency
3.1) Central Tendency: Middle point of a distribution
3.2) Dispersion is spread of data (level to which the values are scattered
3.3) skewness: (Symmetrical or skewed). Skewed to left means long tail is on left side.
3.4) Kurttois: Peakedness. 
3.5) Charactersticts of samples are called statistics
3.6) Charactersticts of population are called parameters

#---------Ungrouped Data starts -------------#
3.7) μ = Σx/N (population) , where (N is number of elements in population)  and (Σx is sum of values of all observations)
3.8) x̄ = Σx/n (sample)
3.9) Disadvantages of Averages
1) impacted by extream values
2) tedious for large values
3) not possible in open interval classes
#---------Ungrouped Data   ends -------------#
 
#---------Grouped Data starts -------------#
3.10) When the number of observations are large, then for ease of calculation some accuracy is given up by gouping values in classes
Step1) We decideon the number of classes in which all values are to be divided.
Step2) We find mid point of each class
Step3) We multiply mid point with frequency of each class, where frequency is the number of values in each class.
x̄ = Σ(frequencyOfEachClass*midPointOfClass)/numberOfObservations             .. here numberOfObservations = ΣfrequencyOfEachClass

Weighted mean given importance to each value based on its weight
x̄weighted = Σ(w*x)/w    .. where w is weight assigned to each observation


as we know .. If  aPOWERn = b  then  a = n√b
Geometric Mean
Sometimes values change over time. We need to know "average rate of change" .. like growth rate. 
GM = n√(a1*a2*a3*.... an)
used with ratios, %change, rate of change




Harmonic mean
square(GM) = AM * HM
AM = (a+b)/2
GM = √(a*b)
HM = 2*ab/(a+b)
HM= n/[(1/r1)+(1/r2)+(1/r3)]

Medean
(n+1)/2

Mode
Mo= LowLimOfModalClass + (d1/(d1+d2))w 
here w is width
d1= frequecy of modal class - frequency of modela class directly below it
d2= frequecy of modal class - frequency of modela class directly above it


Range
diffrece between lowest and highest value
fractile divides all values in 3 parts
quartile divides all values in 4 parts
decile   divides all values in 10 parts

#---------Grouped Data   ends -------------#


DISPERSION
It gives additional information of how reliable is the central tendency value. If the dispersion is wide, then central value is not very strong representative of data

UNGROUPED DATA
σ2 = Σ ( Xi – μ )2 / N
σ = Population Standard Deviation
z Population standard score = (x-μ)/σ

Variance of GROUPED DATA
σ2 = Σf(x-μ)squared/N


SAMPLE Variance
sPower2 = Σ (x-x̄)Power2  / n-1
s = Sample Standard deviation

Standard score tells how many standard deviations away is any value from its mean

SDs of two samples cannot be compared, so we have to convert SD in terms if percentage.
Cofficiant of variation expresses SD as percent of mean. It is a ratio so it does not have a unit.
Population covariance = σ/μ

#------------------------------------------------------------------------------------------------------------------------------

#----------------------  Probability Ch4 starts ------------------------------------------------------
There are two broad types of problems
A) Deterministc (frequency distribution and or discriptive statistics is used)
B) Probabilistic (Random)  (Probability and probability distributions are used)

Sample Space: It is a set of all outcoms
Event: A desirable outcome
Experiment: An activity that produces an Event

There are THREE TYPES OF CLASSIFYING PROBABILITIES
1) CLASSICAL APPROACH (also know as priori)
2) RELARIVE FREQUENCY APPROACH
3) SUBJECTIVE APPROACH

Classical Probability:
Probability of an Event = (number of outcomes where event occured)/total number os events  ...  Here each outcome is equally likely
It is also called prori as we can tell the answer without actually doing experiment. This approach assumes very orderly situations and may not be fit for many real life
Here WITH REPLACEMENT OR WITHOUT REPLACEMENT is important.


RELATIVE FREQUENCY:
If 'f' is the frequency of happening of an event in N trials, then relative frequency = f/N
The observed relative frequecy of an event in very large number of trials. The proportion of times an event occurs in long run under stable conditions.
The relative frequency becoes stable when number of experiments become very large.
Here it is required to do large number of experiments.
The cost of experiments may increase ,so we need to be careful.
In this category, the assumption of all events being equally likely and mutually exclusive is not present.
Disadvantage is that the event should repeat itself after some trials and it is not sure if event stabalizes after some experiments.
In real life, events many not have occured many times and data may not be available to use it.

SUBJECTIVE APPROACH:
These are probabilities assigned to events by person based on educated guess or any evidence available.

P(A) means probability of event A happening.  It is also called marginal or unconditional probability.
ADDITIONAL RULE OF PROBABILISTIC EVENTS If two events are NOT mutually exclusive : P(A or B) = P(A) + P(B) - P(AB)  .. This -P(AB) is to remove double count
ADDITIONAL RULE OF PROBABILISTIC EVENTS If two events ARE mutually exclusive : P(A or B) = P(A) + P(B)

#----STATISTICAL INDEPENDENCE  (conditional,joint, marginal) starts------------------------------------------------
MARGINAL PROBABILITIES UNDER STATISTICAL INDEPENDENCE:
P(A) , as it has no impact of previous or any other event

JOINT PROBABILITIES UNDER STATISTICAL INDEPENDENCE:
The probability of two or more events happening together or in succession is product of their marginal probability.
Joint probability of two independent events:  P(AB) = P(A) * P(B)  ... Here P(AB) is probability of event A and B happening together or in succession.
e.g probability of getting 3heads is 0.5*0.5*0.5 = 0.125

CONDITIONAL PROBABILITIES UNDER STATSTICAL INDEPENDENCE:
P(B|A)  .. Probability of event B given that event A has happened
Conditional: P(B|A) = P(B) if B and A are two independent events
#----STATISTICAL INDEPENDENCE  (conditional,joint, marginal) ends--------------------------------------------------


#----STATISTICAL DEPENDENCE  (conditional,joint, marginal) starts-----------------------------------------------------------
CONDITIONAL PROBABILITIES UNDER STATSTICAL DEPENDENCE:  
P(B|A) = P(BA)/P(A) .... This means probability of B happening given A has occured is joint probability of AB divided by probability of A

JOINT PROBABILITIES UNDER STATSTICAL DEPENDENCE:
taking reference from formula of CONDITIONAL PROBABILITIES UNDER STATSTICAL DEPENDENCE, which is  P(B|A) = P(BA)/P(A), we get --> P(BA) = P(B|A)*P(A)

MARGINAL PROBABILITIES UNDER STATSTICAL DEPENDENCE:
P(A)
#----STATISTICAL DEPENDENCE   (conditional,joint, marginal)  ends-----------------------------------------------------------

BAYES Theorem:
offers powerful statistical method of evaluating new information and revising prior estimates.
P(B|A) = P(BA)/P(A)   .. same as CONDITIONAL PROBABILITIES UNDER STATSTICAL DEPENDENCE..

#----------------------  Probability Ch4   ends ------------------------------------------------------

#----------------------- Probability Distribution Ch 5 Starts --------------------------------------------
Probability distribution is linked to frequency distribution. 
Probability distribution can be thought to be theoritical frequency distribution.
A frequency distribution is listing of "observed" frequencies of all outomes of an experiment that "actually" occured, whereas
a probability distribution is a listing of all "possible" outcomes that "could" result if the experiment was "actually" done.

Probability ditributions are classified as DISCRETE or CONTINOUS
In discrete distribution, the number of possible output are fixed values and hence all possible values can be listed
In continous distribution, infinite number of outputs are possible, so these can not be listed.
Continous distribitions are convenient way of representation of discrete distributions that have millions of possible values.

RANDOM VARIABLE:
A varible that takes different values as a result of outcome of a random experiment.
If this variable is allowed to take only limited values, then it is called discrete random variable
If this variable is allowed to take any value wihin a range, then it is called continous random variable.
 
PROBABILITY DISTRIBUTION OF DISCRETE RANDOM VARIABLE:
The frequency of various outomes (like number of patients screened per day) for say last 100 days is considered as typical, 
we can use these historical records to assign a probability to each frequency and get a probability distribution. Here we
assume that the trend of these 100 days will repeat.

Expected value of discrete random variable is obtained by multiplying each outcome by its %probability and summing it.
Expected value of a discrete random variable is weighted average of each possible outcome, multiplied by probability of
that outcome.

USE OF EXPECTED VALUE IN DECISION MAKING
cherry box and each day's demand example

BINOMIAL DISTRIBUTION (BERNOULLI PROCESS):
Used for discrete (not continous) random variables. 

USE OF BERNOULLI PROCESS:
1) Each trial has only 2 outcomes
2) Probability of the outcome of any trial remains fixed over time
3) Trials are statistically independent.
p = probability of success
q = probability of failure
r = number of successes desired
n = number of trial 

Probability of r success in n trials  = [(n)!/r!*(n-r)!] * (p power(r)) * (q power (n-r))

There is a binomial probability table where for p,n,r combination, the probability is found...

CENTRAL TENDENCY AND DISPERSION FOR BINOMIAL DISTRIBUTION:
Mean of binomial distibution -->  μ = np
σ = √npq


POISSION DISTRIBUTION:
[(𝝺 power x ) * (e power -𝝺) ]/ x!



NORMAL DISTRIBUTION:
Continous random variable distribution.
Properties of Normal Distribution:
	Has single peak
	Mean, Median and Mode overlap
	Two tails never touch x 

There are FAMILIES of Normal curves ...  not just one.
μ and  σ are needed to describe a normal distribution

NOrmal distribution can be approximated for binomial where n is up to 20 and both np and nq are atleast 5


#----------------------- Probability Distribution Ch 5 Ends   --------------------------------------------


#---------------------- Sampling and Sampling distribution starts ------------------------------------------
when all population elements are studied, it is called census. or complete enumeration
Sampeling is used when al elements can not be studied.
Sample is a portion of population
Sample----statstics is characterstics of sample and Parameter is characterstics of population.
Two types of Sampeling (NonRandom OR Judgemental)    AND   (Random or probability)

Biased Samples: When sample is selected that is not proper representaton of population
There are 4 types of Random Sampling:
a) Simple Random sampling: each sample has equal probability of selection and each element has equal prob of getting selected in a sample
b) systematic random sampling: Selected at uniform interval.. may have low cost BUT may inject bias.
c) Stratified sampling:Population is divided into homogenous groups called strata. 	
d) clustered sampling


If we take various samples from a populaton and find the mean and sd of each sample. then the sd and mean of each would be different. A probability distribution of
"all possible means" of the "samples" is a "distribution of sample mean".  It is called sampling distribution of mean.

We can also find sampling distribution of proportions.
STANDARD ERROR:
Standard deviation of distribution of standard means is called standard error of means.
Siilarly, standard deviation of distribution of proportions  is called standard error of proportion.
Standard error gives an idea about that accuracy we are likely to get if we use sample statistices to estimate population parameters.
e.g (standard error of mean, standard error of proportion, standard error of median

SAMPLING FROM NORMAL POPULATIONS:
Say the population mean = 100 and SD =25
If we take samples of 5 items each, then the chances of sample mean to be above population mean is equal to chance of it beimg below population mean.
We would get less spread in sample means then the spread of individual items.
In other words standard error of "Sampling distribution of means" is less than standard deviation of individual items in the population.




#---------------------- Sampling and Sampling distribution   ends ------------------------------------------




















  