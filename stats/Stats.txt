1)
2) Grouping and displaying
2.1) Data Array
2.2) Frequency distribution is a better way of data display w.r.t arrays
2.2.1) Classes of some interval are created and the count of elements (frequency) is callacualed
2.2.2) Relative frequency distribution has the % or values of class, and add to 100% for all classes
2.2.2.1) Steps for frequency distribution
a) Decide number of classes
b) calculate the width of class
c) sort the values and count number of entries per class
d) Histograms and Frquency polygones.. Ogives show relative frequency


3) Measures of entral Tendency and Dispersion in Frequency
3.1) Central Tendency: Middle point of a distribution
3.2) Dispersion is spread of data (level to which the values are scattered
3.3) skewness: (Symmetrical or skewed). Skewed to left means long tail is on left side.
3.4) Kurttois: Peakedness. 
3.5) Charactersticts of samples are called statistics
3.6) Charactersticts of population are called parameters

#---------Ungrouped Data starts -------------#
3.7) μ = Σx/N (population) , where (N is number of elements in population)  and (Σx is sum of values of all observations)
3.8) x̄ = Σx/n (sample)
3.9) Disadvantages of Averages
1) impacted by extream values
2) tedious for large values
3) not possible in open interval classes
#---------Ungrouped Data   ends -------------#
 
#---------Grouped Data starts -------------#
3.10) When the number of observations are large, then for ease of calculation some accuracy is given up by gouping values in classes
Step1) We decideon the number of classes in which all values are to be divided.
Step2) We find mid point of each class
Step3) We multiply mid point with frequency of each class, where frequency is the number of values in each class.
x̄ = Σ(frequencyOfEachClass*midPointOfClass)/numberOfObservations             .. here numberOfObservations = ΣfrequencyOfEachClass

Weighted mean given importance to each value based on its weight
x̄weighted = Σ(w*x)/w    .. where w is weight assigned to each observation


as we know .. If  aPOWERn = b  then  a = n√b
Geometric Mean
Sometimes values change over time. We need to know "average rate of change" .. like growth rate. 
GM = n√(a1*a2*a3*.... an)
used with ratios, %change, rate of change




Harmonic mean
square(GM) = AM * HM
AM = (a+b)/2
GM = √(a*b)
HM = 2*ab/(a+b)
HM= n/[(1/r1)+(1/r2)+(1/r3)]

Medean
(n+1)/2

Mode
Mo= LowLimOfModalClass + (d1/(d1+d2))w 
here w is width
d1= frequecy of modal class - frequency of modela class directly below it
d2= frequecy of modal class - frequency of modela class directly above it


Range
diffrece between lowest and highest value
fractile divides all values in 3 parts
quartile divides all values in 4 parts
decile   divides all values in 10 parts

#---------Grouped Data   ends -------------#


DISPERSION
It gives additional information of how reliable is the central tendency value. If the dispersion is wide, then central value is not very strong representative of data

UNGROUPED DATA
σ2 = Σ ( Xi – μ )2 / N
σ = Population Standard Deviation
z Population standard score = (x-μ)/σ

Variance of GROUPED DATA
σ2 = Σf(x-μ)squared/N


SAMPLE Variance
sPower2 = Σ (x-x̄)Power2  / n-1
s = Sample Standard deviation

Standard score tells how many standard deviations away is any value from its mean

SDs of two samples cannot be compared, so we have to convert SD in terms if percentage.
Cofficiant of variation expresses SD as percent of mean. It is a ratio so it does not have a unit.
Population covariance = σ/μ

#------------------------------------------------------------------------------------------------------------------------------

#----------------------  Probability starts ------------------------------------------------------
There are two broad types of PROBLEMS
A) Deterministc (frequency distribution or discriptive statistics is used)
B) Probabilistic (Random)  (Probability and probability distributions are used)

Sample Space: It is a set of all outcoms
Event: A desirable outcome
Experiment: An activity that produces an Event
Mutually Exclusive: When only one of the events can occur, and other can not.
Collectively Exhaustive: All possible outcomes

There are THREE TYPES OF CLASSIFYING PROBABILITIES
1) CLASSICAL APPROACH (also know as priori)
2) RELARIVE FREQUENCY APPROACH
3) SUBJECTIVE APPROACH

Classical Probability:
Probability of an Event = (number of outcomes where event occured)/total number os events  ...  Here each outcome is equally likely
It is also called prori as we can tell the answer without actually doing experiment. This approach assumes very orderly situations and may not be fit for many real life
Here WITH REPLACEMENT OR WITHOUT REPLACEMENT is important.


RELATIVE FREQUENCY:
If an even is capable of repeating itself with a frequency of 'f' in sufficiently large(N) trials, then relative frequency = f/N
The observed relative frequecy of an event in very large number of trials. The proportion of times an event occurs in long run under stable conditions.
The relative frequency becomes stable when number of experiments become very large.
Here it is required to do large number of experiments.
The cost of experiments may increase ,so we need to be careful.
In this category, the assumption of all events being equally likely and mutually exclusive is not present.
Disadvantage is that this approach assumes the event should repeat itself after some trials and the environmental conditions should be same(which is not realistic).
It is also not possble to know that after how many occurances, the event stabalizes.
In real life, events many not have occured many times and data may not be available to use it.

SUBJECTIVE APPROACH:
These are probabilities assigned to events by person based on educated guess or any evidence available.


P(A) means probability of event A happening.  It is also called marginal or unconditional probability.
ADDITIONAL RULE OF PROBABILISTIC EVENTS If two events are NOT mutually exclusive : P(A or B) = P(A) + P(B) - P(AB)  .. This -P(AB) is to remove double count
ADDITIONAL RULE OF PROBABILISTIC EVENTS If two events ARE mutually exclusive : P(A or B) = P(A) + P(B)

#----STATISTICAL INDEPENDENCE  (conditional,joint, marginal) starts------------------------------------------------
MARGINAL PROBABILITIES UNDER STATISTICAL INDEPENDENCE:
P(A) , as it has no impact of previous or any other event

JOINT PROBABILITIES UNDER STATISTICAL INDEPENDENCE:
The probability of two or more events happening together or in succession is product of their marginal probability.
Joint probability of two independent events:  P(AB) = P(A) * P(B)  ... Here P(AB) is probability of event A and B happening together or in succession.
e.g probability of getting 3heads is 0.5*0.5*0.5 = 0.125

CONDITIONAL PROBABILITIES UNDER STATSTICAL INDEPENDENCE:
P(B|A)  .. Probability of event B given that event A has happened
Conditional: P(B|A) = P(B) if B and A are two independent events
#----STATISTICAL INDEPENDENCE  (conditional,joint, marginal) ends--------------------------------------------------


#----STATISTICAL DEPENDENCE  (conditional,joint, marginal) starts-----------------------------------------------------------
CONDITIONAL PROBABILITIES UNDER STATSTICAL DEPENDENCE:  
P(B|A) = P(BA)/P(A) .... This means probability of B happening given A has occured is joint probability of AB divided by probability of A

JOINT PROBABILITIES UNDER STATSTICAL DEPENDENCE:
taking reference from formula of CONDITIONAL PROBABILITIES UNDER STATSTICAL DEPENDENCE, which is  P(B|A) = P(BA)/P(A), we get --> P(BA) = P(B|A)*P(A)

MARGINAL PROBABILITIES UNDER STATSTICAL DEPENDENCE:
P(A) ... Sum all probabilities of joint events where A happened.
#----STATISTICAL DEPENDENCE   (conditional,joint, marginal)  ends-----------------------------------------------------------

BAYES Theorem:
offers powerful statistical method of evaluating new information and revising prior estimates.
P(B|A) = P(BA)/P(A)   .. same as CONDITIONAL PROBABILITIES UNDER STATSTICAL DEPENDENCE..

REVISING PRIOR ESTIMATES WITH BAYES THEOREM
Additional information may result in need to revise probabilities. These new probabilites are called POSTERIOR probabilities
It makes possible to avoid collection of massive data over period of time to make good decision based on probability.
  
example: two die with .4 and .7 chances of showing 1 dot
#----------------------  Probability ends ------------------------------------------------------

#----------------------- Probability Distribution Starts --------------------------------------------
Probability distribution can be thought to be theoritical frequency distribution. A theoritical frequency distribution is how outputs are EXPECTED to appear.
(example , if we take two coin and plot the frequency distribution of H, then (TT,TH,HT,HH) for 0(1occurance),1(2occurance),2(1occurance) is probability distribution.
That means, a probability distribution is a listing of all "possible" outcomes that "could" result if the experiment was "actually" done.

Probability distribution is linked to frequency distribution. 
A frequency distribution is listing of ALL "observed" frequencies of all outomes of an experiment that "actually" occured, whereas probability distribution is a listing
of probabilities of all possible outcomes that COULD theoritically result.


TYPES OF PROBABILITY DISTRIBUTIONS:
Probability ditributions are classified as DISCRETE or CONTINOUS
In discrete distribution, the number of possible output are fixed values and hence all possible values can be listed

In continous distribution, infinite number of outputs are possible, so these can not be listed.
Continous distribitions are convenient way of representation of discrete distributions that have millions of possible values.

RANDOM VARIABLE:
A varible that takes different values as a result of outcome of a random experiment in NO PREDICTABLE SEQUENCE.
If this variable is allowed to take only limited values, then it is called discrete random variable
If this variable is allowed to take any value wihin a range, then it is called continous random variable.

 
PROBABILITY DISTRIBUTION OF DISCRETE RANDOM VARIABLE:
The frequency of various outomes (like number of patients screened per day) for say last 100 days is considered as typical, 
we can use these historical records to assign a probability to each frequency and get a probability distribution. Here we
assume that the trend of these 100 days will repeat. Here 100 observaions are taken. The frequency of each value of "number of patients" is used to get %age of that
number and sumtotal of all these frequencies is = 100

Expected value of discrete random variable is obtained by multiplying each outcome by its %probability and summing it.
Expected value of a discrete random variable is weighted average of each possible outcome, multiplied by probability of
that outcome.

USE OF EXPECTED VALUE IN DECISION MAKING
cherry box and each day's demand example


BINOMIAL DISTRIBUTION (BERNOULLI PROCESS):
Used for discrete (not continous) random variables. 

CONDITIONS WHEN BERNOULLI PROCESS CAN BE USED:
1) Each trial has only 2 outcomes
2) Probability of the outcome of any trial remains fixed over time
3) Trials are statistically independent.
p = probability of success
q = probability of failure
r = number of successes desired
n = number of trial 

Probability of r success in n trials  = [(n)!/r!*(n-r)!] * (p power(r)) * (q power (n-r))
(example: 0,1,2,3,4,5 employees coming late)

Graphical observations of binomial distribution:
a) when p is small (.1), the distribution is scewed to right (long tail at right)
b) when p increases to .3, the scew is less noticable
c) when p=.5, the distribution is symetrical
d) when p is large ,  the distribution is scewed to left

There is a binomial probability TABLE where for p,n,r combination, the probability is found...

CENTRAL TENDENCY AND DISPERSION FOR BINOMIAL DISTRIBUTION:
Mean of binomial distibution -->  μ = np
σ = √(npq)


POISSION DISTRIBUTION:


(example: number of patients arriving in an interval of time)
properties of POISSON distribution
a) mean number of events occuring per interval of time can be obtained from historic data
b) If the time is divided into very small interval(say 1 sec) then below are true:
b.1) The probability of event happening per sec(small interval) is constant and only 1 event can happen in this interval (e.g only 1 patient can come per sec)
b.2) The probability of 2 patiests comming per sec is so small that it is assigned 0
b.3) The number of patients coming per sec are independent of when that 1 sec happens in full duration of 1 hr
b.4) The number of patients arriving per sec are independent of number of patients arriving at any other second

p(x) =  [(𝝺 power x ) * (e power -𝝺) ]/ x!

p(x) = probability of exactly x occcurances
𝝺 = mean number of occurances per unit time 
(
ex: if 5 accidents happen per month at a crossing, then 𝝺 =5 and x=0
probability of 0 accident = p(0) = [ (5 power 0)*(e power -5)]/0!
probability of 2 accident = p(2) = [ (5 power 2)*(e power -5)]/2! 
)


NORMAL DISTRIBUTION:
Continous random variable distribution.
Properties of Normal Distribution:
	Has single peak
	Mean, Median and Mode overlap
	Two tails never touch x 

There are FAMILIES of Normal curves ...  not just one.
μ and  σ are needed to describe a normal distribution


SOME QUICK POINTS
a) for binomial distribution, if (n =>20 AND np <=0.5, then POISSON can be approximated
b) for binomial distribution, if (np =>5 AND nq <= 5, then Normal distribution cane be used with μ=np and σ = √(npq)



#----------------------- Probability Distribution Ends   --------------------------------------------


#---------------------- Sampling and Sampling distribution starts ------------------------------------------
when all population elements are studied, it is called census. or complete enumeration
Sampeling is used when al elements can not be studied.
Sample is a portion of population
Sample----statstics is characterstics of sample and Parameter is characterstics of population.
Two types of Sampeling (NonRandom OR Judgemental)    AND   (Random or probability)

Biased Samples: When sample is selected that is not proper representaton of population
There are 4 types of Random Sampling:
a) Simple Random sampling: each sample has equal probability of selection and each element has equal prob of getting selected in a sample
b) systematic random sampling: Selected at uniform interval.. may have low cost BUT may inject bias.
c) Stratified sampling:Population is divided into homogenous groups called strata. 	
d) clustered sampling


If we take various samples from a populaton and find the mean and sd of each sample. then the sd and mean of each would be different. A probability distribution of
"all possible means" of the "samples" is a "distribution of sample mean".  It is called sampling distribution of mean.

We can also find sampling distribution of proportions.
STANDARD ERROR:
Standard deviation of distribution of standard means is called standard error of means.
Siilarly, standard deviation of distribution of proportions  is called standard error of proportion.
Standard error gives an idea about that accuracy we are likely to get if we use sample statistices to estimate population parameters.
e.g (standard error of mean, standard error of proportion, standard error of median

SAMPLING FROM NORMAL POPULATIONS:
Say the population mean = 100 and SD =25
If we take samples of 5 items each, then the chances of sample mean to be above population mean is equal to chance of it beimg below population mean.
We would get less spread in sample means then the spread of individual items.
In other words standard error of "Sampling distribution of means" is less than standard deviation of individual items in the population.




#---------------------- Sampling and Sampling distribution   ends ------------------------------------------




















  