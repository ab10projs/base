7-- Introduction
1) At the core of Azure Databricks is opensource Distributed compute processing engine called Apache spark
2) Databricks is a company created by founders of Apache Spark to make it easier to work with Spark
3) Azure databricks --> Spark, Databricks, Azure
	3.1) Spark: Distributed compute processing engine (yahoo, ebay, netflick) multiple petabytes of data, 1000s of nodes
			: 100 times faster than Hadoop , has in-memory
			: Runs on distributed computed platform
			: Unified engine which supports SQL, Streaming, ML, etc..
			: Spark core (RDD)<---> Python, R ...., takes care taskSchedling,MmryMgmt., fault rec, StorageInterction, Main programming abstraction 				API called RDD... RDDs are collection of various compute items distributed across various nodes
			: Spark introduced SQL engine that has (Catalyst Optimizer and Tungsten)
			: Catalyst Optimized: converts a computational query--> highly optimized execution plan.
			: Tungsten: Memory Management and CPU efficiency. 
			: High level abstractions like DF APIs make it easier to develop apps, So use DF API, as it uses SQL optimizations, rather than RDDAPI
			: Spark has a standalone resource manager, But you can chose other resource managers like Yarn, Apache Mesos, Kubernetes

	3.2) Databricks	: Makes it easier to work with spark, as to work with Spark we need to manage clusters, security, 3rd party products.
			: We need to create cluster and install software to work with spark.
			: Databricks gives to ability to create clusters in a few clicks. chose runtime 
			: Has Jupyter type Notebooks
			: Provides Administration controls
			: Optimized runtime (5x faster than vanilla Apache Spark), and ability to create tables	
			: Open Source Project Delta Lake for Analysis, and also SQL Analytics, ML flow
			: Also provides integration with other Microsoft services
				
7-- Introduction
8-- Create Databricks Sevice in Azure
	Azure Portal
		-- Create Resource
			-- Azuere databrics
				-- create resourceGroup, workspace. 
				-- Network , take no options
				-- leave encryption to default
				-- tag 
				-- Deploy and check status.. in 4 minutes it will become active... 
				-- Pin and then new dashboard Launch workspace.
8-- Create Databricks Service in Azure

9-- Databricks User Interface Overview
	-- At this stage, from home you will see Databricks workspace (dbc-ws) that you created. Its type is Azure Databricks Services. click on it.
	-- After launching the databricks, you will see menu items.
	-- left Nav has multiple products (Data Warehousing, Data Engineering and ML)
	-- Top part has common functionalities.
	-- SQL has Databricks SQL... For SQL Data Warehouses on Lakehouse
	-- Data Engineering and ML
	-- New (Notebook, Query, Dashboard, job, DTL Pipeline, Alert, Model )
	-- Workspace is a container for holding folders and files. 
		-- each user has a workspace and a shared workspace also  
		-- Repo -- for GIT and other version control
	-- Catalog is for creating tables in databricks
	-- Workflows .. jobs, Jobruns, Delta live tables (ETL)
	-- Compute .. to create clusters
	-- Data Engineering has repeate of Workflows
	-- Advance serach at top searches everywhere in workspace.. You can also switch workspaces.
9-- Databricks User Interface Overview

10-- Azure Databricks Architecture
	-- Control Plane (Databricks Subscription) {clusterManager, Databrick UX, DBFS, Metadata about clusters, files mounted}
	-- Data Plane (Customer Subscription) 4 resources created {VertualNetwork-VNet, NetworkSecurityGroup,NSG, AzureBlockStorage,DatabricksWorkspace}
	-- Our data always resides and processed in our subscription
10-- Azure Databricks Architecture

11-- Databricks Clusters
	-- what is cluster
	-- two types of clusters
	-- configuration options
	-- create cluster
	-- pricing
	-- cost control
	-- cluster pools
	-- cluster policy
11-- Databricks Clusters

13-- Azure Databricks cluster types
	-- A collection of vms
	-- One driver node and multiple worker nodes
	-- cluster allows us to treat these nodes as a single computer engine by driver node.
	-- Two types (ALL purpose) and (Job Cluster) 
		-- All purpose
			-- created manually by GUI/CLI or API
			-- persistent, can be terminated and restarted at any point of time
			-- suitable for interactive workloads
			-- can be shared among users
			-- expensive
		-- Job Cluster
			-- created by jobs when job starts to execute and job is configured to use job cluster
			-- Terminated at end of the job. they can not be restarted. No longer reusable once job is completed.
			-- suitable for automated work loads
			-- isolated, just for the job
			-- cheap
	-- how to create cluster
		-- go to cluster and launch it.
			-- click on Compute
				-- you will see 4 tabs (all purpose, job, policies, pool)
				-- cluster pools give to ready to use compute capacity to create all purpose cluster quickly.
				-- usually it take 5 to 6 minutes to spinup a cluser, if you have a pool or resources, then it can be quick.
				-- There are a lot of cluster configuration settings. Policies help preconfigure some of these settings and can
					control the max size and cost of cluster under control
13-- Azure Databricks cluster types


14-- Azure Databricks cluster configuration
	-- single/ multi node cluster  
	-- Multi (horizontally scale, suitable for large workloads)
	-- access mode
		-- Single user (only 1 user)
		-- Shared (only python and sql workloads).. Provides process isolation, each process gets its environment, so one process can't see data 
			of other process.
		-- No Isolation Shared ... it doesn't provide any process isolation. Failure in one user process may effect other users.
	-- Runtime libraries
		-- set of core libraries that run on databrick cluster. There are 4 types of runtimes
			-- Databricks Runtime (spark, Scala, R, Python, Ubuntu, GPU, Delta lake)
			-- Databricks Runtime ML (Databricks Runtime + ML)
			-- Photon Runtime (Databricks Runtime + Photon engine for faster SQL )
			-- Databricks Runtime Light (for automated workloads)
	-- Auto Termination
		-- cost control.. 
		-- terminate cluster after x minutes of use.
		-- specify default minutes of use to avoid running when not needed
	-- Auto Scaling (min and max worker nodes)
		-- add or remove worker nodes depending on workload
	-- Cluster VM Types/ Size  (many easy to uderstand groups)
		-- Memory optimized Instance type .. for memory intensive .. e.g ML workload that caches lot of data in memory
		-- Compute optimized Instance types .. where processing rate > input rate.. Also for distributed analytics and data science
		-- Storage optimized (high disk throughput and IO). 
		-- General purpose (in memory cashing)
		-- GPU
	-- Cluster policy
		-- It simplifies user interface
		-- No need for administrators
		-- cost control, by restricting max size of cluster
14-- Azure Databricks cluster configuration

15-- Create cluser in Azure Databricks
	-- Edit the name as ClusterGivenName
	-- Policy --> Unrestricted
15-- Create cluser in Azure Databricks

16-- Azure Databricks Pricing
	-- Depends on many factors
		-- type of workload (All Purpose, Job, SQL, Photon)
		-- Tier (premium or standard)
		-- vm type (general purpose/GPU/Optimized)
		-- Actual price .. look for two things (DBUs and VMs, count worker and master nodes)... small charges for IP, 
		-- Set budget alert
			
16-- Azure Databricks Pricing

20-- Databricks Notebook
	-- what is a notebook
	-- creating a notebook
	-- Magic commands
	-- Databricks Utilities
	-- Import project solution Notebooks
20-- Databricks Notebook

21-- Databricks Notebooks creation
	-- launch workspace
		-- go to user/ create a new folder and name it dbc-course
		-- start the cluster from compute
21-- Databricks Notebooks creation

22-- Magic commands
	-- %sql
	-- %fs
	   ls	
	-- %md
	-- %sh
	   ps 
22-- Magic commands

23-- Utilities
	-- combine different types of tasks in single notebook
	-- File system utilities
		-- databricls filesystem from notebook
	-- Secret utilities
		-- Azure key vault
	-- Widget utilities
		-- to pass parameters at runtime in pipeline, making notebook reusable
	-- Notebook work utilities
		-- Invoke one notebook from another and chain notebooks

		-- 	%fs
			ls /databricks-datasets
			----	
			dbutils.fs.ls('/')
			----
			for i in dbutils.fs.ls('/databricks-datasets/COVID/'):
    				if i.name.endswith('/'):
        				print(i.name)
			----
			dbutils.help()
			----
			dbutils.fs.help('ls')
			----
23-- Utilities


24-- import dbc files
	--- 
24-- import dbc files
